<!DOCTYPE HTML>
<html lang="en">
  <head>
    <style>
    /* code.language-bash {
      background-color: #f4f4f4;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 2px 5px;
      font-family: monospace;
    } */
    code.language-bash {
      background-color: #282c34;
      color: #ffffff;
      border: 1px solid #3e4451;
      border-radius: 4px;
      padding: 4px 8px;
      font-family: 'Courier New', Courier, monospace;
      font-size: 0.9em;
      display: inline-block;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
  </style>
      <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QG8W74H2DV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QG8W74H2DV');
</script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Walid Bousselham</title>

    <meta name="author" content="Walid Bousselham">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <style>
      body {
        font-family: 'Roboto', Arial, sans-serif;
        line-height: 1.6;
      }
      h1, h2, .name {
        font-family: 'Montserrat', sans-serif;
      }
    </style>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&family=Montserrat:wght@500;700&display=swap" rel="stylesheet">
    
  </head>


  <body style="background-color: #f9f9f9;">
    <style>
      html {
        scroll-behavior: smooth;
      }
    </style>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Walid Bousselham
                </p>
                <p>I'm a PhD student at <a href="https://tuebingen.ai/">T√ºbingen AI Center</a>, advised by <a href="https://hildekuehne.github.io/">Prof. Hilde Kuehne</a>.
                  I'm also participating in <a href="https://sightandsound.csail.mit.edu/"> MIT-IBM Watson Sight and Sound Project</a>.
                </p>
                  <p>
                    My primary research area is deep learning for multimodal models. I am interested in various aspects of these models, ranging from improving their pretraining processes and understanding their internal prediction mechanisms to exploring zero-shot adaptation capabilities.
                </p>
                <p> Prior to this, I finished my Master of Engineering in Applied Mathematics at <a href="https://www.ensta-paris.fr/en/node"> ENSTA Paris</a> in France
                  and my Master of Science in Statistics and applied Probabilities at the   <a href="https://www.stat.nus.edu.sg/"> National University of Singapore (NUS) </a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:walidbousselham@gmail.com">Email</a> &nbsp;/&nbsp;
<!--                  <a href="data/WalidBousselham-CV.pdf">CV</a> &nbsp;/&nbsp;-->
                  <a href="https://scholar.google.com/citations?user=vbx_PS0AAAAJ&hl=fr">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/BousselhamWalid">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/WalBouss">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Walid_Bousselham_profile_picture.jpg">
                  <img class="profile-pic" style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Walid_Bousselham_profile_picture.jpg" class="hoverZoomLink">
                </a>
              </td>
            </tr>
          </tbody></table>

<!--            News: -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>üî• News</h2>
                <div class="news-item">
                  <p><strong> 05.2024 </strong>&ensp; I spend the summer 2024 at MiT CSAIL as a visiting scholar working with <a href="http://hendrik.strobelt.com/">Hendrik Strobelt</a> and  <a href="http://angieboggust.com/">Angie Boggust</a>. </p>
                <!-- </div> -->
                <!-- <div class="news-item"> -->
                  <p><strong> 05.2024 </strong>&ensp; I gave a talk at "<a href="https://cohere.com/events/c4ai-Walid-Bousselham-2024">Cohere For AI - Community Talks</a>" regarding our latest work <a href="http://walidbousselham.com/LeGrad/">"LeGrad"</a> in collaboration with MiT & IBM Research. </p>
                  <p><strong> 03.2024 </strong>&ensp; Our paper <a href="https://arxiv.org/abs/2312.00878">Grounding Everything: Emerging Localization Properties in Vision-Language Transformers</a> was accepted at <strong>CVPR 2024!</strong></a>. </p>
                  <p><strong> 01.2024 </strong>&ensp; I gave an interview to the <strong>Computer Vision News magazine</strong>, that features our recent paper <a href="https://github.com/WalBouss/GEM">"Grounding Everything"</a>. <strong>[<a href="https://www.rsipvision.com/ComputerVisionNews-2024January/6/">Link to the interview</a>] </strong> </p>
                  <p><strong> 01.2024 </strong>&ensp; I will be attending the <a href="https://www.bmva.org/meetings/24-01-17-Vision%20and%20Language.html">BMVA Symposium on Vision and Language</a> with an oral and a poster presenting our recent paper <a href="https://github.com/WalBouss/GEM">Grounding Everything</a>. </p>
                </div>
                </td>
              </tr>
          </tbody></table>

          <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>üî¨ Featured Research</h2>

              </td>
            </tr>
          </tbody></table>
          <table style="width:140%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!-- Dex-AR Paper         -->
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle"> <img src='images/DEX-AR_main_figure.jpg' width="260"></td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://walidbousselham.com/">
      <span class="papertitle">DEX-AR: A Dynamic Explainability Method for Autoregressive Vision-Language Models
</span>
    </a>
    <br>
	 <strong>Walid Bousselham</strong>,
      <a href="http://angieboggust.com/">Angie Boggust</a>,
	<a href="http://hendrik.strobelt.com/">Hendrik Strobelt</a>,
	 <a href="https://hildekuehne.github.io/"> Hilde Kuehne</a>
    <br>
	<em>arXiv</em>, 2025
    <br>
      <a href="https://walidbousselham.com/">(coming soon on ArXiv)</a>
    </br>
      </br>
  </td>
</tr>
          <!-- Dex-AR Paper: End         -->

          <!-- MaskInversion Paper         -->
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle"> <img src='images/maskinversion_main_figure.jpg' width="260"></td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://walidbousselham.com/MaskInversion">
      <span class="papertitle">MaskInversion: Localized Embeddings via Optimization of Explainability Maps
</span>
    </a>
    <br>
	 <strong>Walid Bousselham</strong>,
      <a href="https://scholar.google.com/citations?user=8tewdk4AAAAJ&hl">Sofian Chaybouti</a>,
      <a href="https://chrirupp.github.io/"> Christian Rupprecht</a>,
      <a href="https://sites.google.com/view/vittoferrari">Vittorio Ferrari</a>,
	 <a href="https://hildekuehne.github.io/"> Hilde Kuehne</a>
    <br>
	<em>arXiv</em>, 2024
    <br>
      <a href="https://walidbousselham.com/MaskInversion">Project Page</a>
      /
    <a href="https://github.com/WalBouss/MaskInversion">Code</a>
      /
      <a href="https://arxiv.org/abs/2407.20034">arXiv</a>
<!--        /-->
<!--        <a href="https://huggingface.co/spaces/WalidBouss/MaskInversion">Demo</a>-->
    </br>
      </br>
  </td>
</tr>
          <!-- MaskInversion Paper: End         -->


                    <!-- LeGrad Paper         -->
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle"> <img src='images/LeGrad_main_figure.jpg' width="260"></td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://walidbousselham.com/LeGrad">
      <span class="papertitle">LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity
</span>
    </a>
    <br>
	 <strong>Walid Bousselham</strong>,
      <a href="http://angieboggust.com/">Angie Boggust</a>,
      <a href="https://scholar.google.com/citations?user=8tewdk4AAAAJ&hl">Sofian Chaybouti</a>,
	<a href="http://hendrik.strobelt.com/">Hendrik Strobelt</a>,
	 <a href="https://hildekuehne.github.io/"> Hilde Kuehne</a>
    <br>
	<em>arXiv</em>, 2024
    <br>
      <a href="https://walidbousselham.com/LeGrad">Project Page</a>
      /
    <a href="https://github.com/WalBouss/LeGrad">Code</a>
      /
      <a href="https://arxiv.org/abs/2404.03214">arXiv</a>
        /
        <a href="https://huggingface.co/spaces/WalidBouss/LeGrad">Demo</a>
    </br>
      </br>
  </td>
</tr>
          <!-- LeGrad Paper: End         -->

          <!-- GEM Paper         -->
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle"> <img src='images/GEM_main_figure.jpg' width="260"></td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2312.00878">
      <span class="papertitle">Grounding Everything: Emerging Localization Properties in Vision-Language Transformers
</span>
    </a>
    <br>
	 <strong>Walid Bousselham</strong>,
      <a href="https://petersen.ai/"> Felix Petersen</a>,
	<a href="https://sites.google.com/view/vittoferrari">Vittorio Ferrari</a>,
	 <a href="https://hildekuehne.github.io/"> Hilde Kuehne</a>
    <br>
	<em>CVPR</em>, 2024
    <br>
    <a href="https://github.com/WalBouss/GEM">Code</a>
      /
      <a href="https://arxiv.org/abs/2312.00878">arXiv</a>
        /
        <a href="https://huggingface.co/spaces/WalidBouss/GEM">Demo</a>
    </br>
      </br>
  </td>
</tr>
          <!-- GEM Paper: End         -->

<!-- HGQA Paper         -->
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle"> <img src='images/HGQA_architecture.png' width="260"></td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Urooj_Learning_Situation_Hyper-Graphs_for_Video_Question_Answering_CVPR_2023_paper.html">
      <span class="papertitle">Learning Situation Hyper-Graphs for Video Question Answering
</span>
    </a>
    <br>
	<a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=ceiuCp4AAAAJ"> Aisha Urooj</a>,
	 <a href="https://hildekuehne.github.io/"> Hilde Kuehne</a>,
	<a href="https://bobbywu.com/">Bo Wu</a>,
	<a href="https://paperswithcode.com/search?q=author%3AKim+Chheu">Kim Chheu</a>,
    <strong>Walid Bousselham</strong>,
	<a href="https://people.csail.mit.edu/ganchuang/"> Chuang Gan</a>,
	<a href="https://www.crcv.ucf.edu/person/niels-lobo/"> Niels Lobo</a>,
    <a href="https://www.crcv.ucf.edu/person/mubarak-shah/"> Mubarak Shah</a>
    <br>
	<em>CVPR</em>, 2023
    <br>
    <a href="https://github.com/aurooj/SHG-VQA">Code</a>
      /
      <a href="https://arxiv.org/abs/2304.08682">arXiv</a>
        </br>
      </br>
  </td>
</td>

<!-- SenFormer Paper         -->
<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle"> <img src='images/SenFormer_main_figure.png' width="260"></td>

  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2111.13280">
      <span class="papertitle">Efficient Self-Ensemble for Semantic Segmentation
</span>
    </a>
    <br>
	<strong>Walid Bousselham</strong>,
	<a href="https://www.thibault.biz/"> Guillaume Thibault</a>,
	 <a href="https://www.ohsu.edu/people/lucas-pagano-bs"> Lucas Pagano</a>,
	<a href="https://scholar.google.com/citations?user=6RVrwWMAAAAJ&hl=en">Archana Machireddy</a>,
	<a href="https://scholar.google.com/citations?user=EEWjJB0AAAAJ&hl=en">Joe Gray</a>,
	<a href="https://sites.google.com/site/yhchangucb"> Young Hwan Chang</a>,
	<a href="https://scholar.google.com/citations?user=Tjs5eWYAAAAJ&hl=en"> Xubo Song</a>
    <br>
	<em>BMVC</em>, 2022
    <br>
    <a href="https://github.com/WalBouss/SenFormer">Code</a>
      /
      <a href="https://arxiv.org/abs/2111.13280">arXiv</a>
      /
      <a href="https://www.youtube.com/watch?v=K83b5WRJ3tM&ab_channel=WalidBousselham">video</a>
    </br>
          </br>
      </td>
    </td>
</tr>

</tbody></table>

          <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));">

<!--    Open-source libraries-->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <h2>üõ†Ô∏è Open-source Libraries</h2>
    </td>
  </tr>
</tbody></table>
<table style="width:140%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!-- MaskInversion Library -->
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle"> <img src='images/maskinversion_library_figure.jpg' width="260"></td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://github.com/WalBouss/MaskInversion">
      <span class="papertitle">MaskInversion</span>
    </a>
    <br>
    A library for generating localized embeddings of CLIP-like models via optimization of explainability maps.
    <br><br>
    <code class="language-bash">pip install maskinversion_torch</code>
    <br><br>
    <a href="https://github.com/WalBouss/MaskInversion">GitHub</a>
    /
    <a href="https://pypi.org/project/maskinversion_torch">PyPI</a>
  </td>
</tr>

<!-- LeGrad Library -->
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div style="display: flex; justify-content: center; align-items: center; height: 100%;">
      <img src='images/LeGrad_logo.jpg' width="180" style="max-width: 100%; height: auto;">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://github.com/WalBouss/LeGrad">
      <span class="papertitle">LeGrad</span>
    </a>
    <br>
    An explainability method for Vision Transformers that, given a text prompt, generates a heatmap localizing the part of the image that is important for the model to recognize the text prompt.
    <br><br>
    <code class="language-bash">pip install legrad_torch</code>
    <br><br>
    <a href="https://github.com/WalBouss/LeGrad">GitHub</a>
    /
    <a href="https://pypi.org/project/legrad_torch">PyPI</a>
  </td>
</tr>

<!-- GEM Library -->
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle"> <img src='images/GEM_logo.jpeg' width="260"></td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://github.com/WalBouss/GEM">
      <span class="papertitle">GEM (Grounding Everything Method)</span>
    </a>
    <br>
    A library for exploring emerging localization properties in Vision-Language Transformers.
    <br><br>
    <code class="language-bash">pip install gem_torch</code>
    <br><br>
    <a href="https://github.com/WalBouss/GEM">GitHub</a>
    /
    <a href="https://pypi.org/project/gem_torch">PyPI</a>
  </td>
</tr>

<!-- Data Stream Library -->
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div style="display: flex; justify-content: center; align-items: center; height: 100%;">
      <img src='images/data_stream_logo.jpeg' width="220" style="max-width: 100%; height: auto;">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://github.com/WalBouss/data-stream">
      <span class="papertitle">Data Stream</span>
    </a>
    <br>
    A Python tool for streaming data from remote servers to local compute resources, particularly useful for training models on large datasets stored remotely without requiring local storage <i>(developed for internal use)</i>.
    <br><br>
    <code class="language-bash">pip install data-streaming</code>
    <br><br>
    <a href="https://github.com/WalBouss/data-stream">GitHub</a>
    /
    <a href="https://pypi.org/project/data-streaming">PyPI</a>
  </td>
</tr>

</tbody></table>
<!--    End:Open-source libraries-->

          <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));">

          <!-- Media Coverage Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>üì∞ Media Coverage</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:140%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <!-- Cohere Talk -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"> 
                <img src='images/cohere_talk.jpg' width="260">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://cohere.com/events/c4ai-Walid-Bousselham-2024">
                  <span class="papertitle">Talk at Cohere For AI - Community Talks</span>
                </a>
                <br>
                Presented our latest work on LeGrad, discussing novel approaches to explainability in Vision Transformers.
                <br><br>
                <a href="https://www.youtube.com/watch?v=LX93vlEImS0&ab_channel=Cohere">Watch Talk</a>
                /
                <a href="http://walidbousselham.com/LeGrad/">LeGrad Project</a>
              </td>
            </tr>

            <!-- Computer Vision News Interview -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"> 
                <img src='images/cv_news_interview_jan_2024.jpeg' width="260">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.rsipvision.com/ComputerVisionNews-2024January/6/">
                  <span class="papertitle">Computer Vision News Magazine Interview</span>
                </a>
                <br>
                Featured interview discussing our paper "Grounding Everything" and its implications for Vision-Language models.
                <br><br>
                <a href="https://www.rsipvision.com/ComputerVisionNews-2024January/6/">Read Interview</a>
                /
                <a href="https://github.com/WalBouss/GEM">GEM Project</a>
              </td>
            </tr>

          </tbody></table>

          <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));">

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design and source code borrowed from <a href="https://jonbarron.info/"> Jon Barron</a>'s website.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
    <style>
      a {
        transition: color 0.3s ease;
      }
      a:hover {
        color: #5bd889;
      }
    </style>
    <style>
      /* News items */
      .news-item:nth-child(even) {
        background-color: #f0f0f0;
      }
      .news-item {
        padding: 15px;
        border-radius: 5px;
        margin-bottom: 10px;
      }

      /* Featured research */
      .research-item {
        display: flex;
        margin-bottom: 30px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        border-radius: 8px;
        overflow: hidden;
      }
      .research-image {
        flex: 0 0 260px;
      }
      .research-content {
        flex: 1;
        padding: 20px;
      }

      /* Profile picture */
      .profile-pic {
        transition: transform 0.3s ease;
      }
      .profile-pic:hover {
        transform: scale(1.05);
      }

      /* Responsive design */
      @media (max-width: 768px) {
        .research-item {
          flex-direction: column;
        }
        .research-image {
          width: 100%;
          max-width: 100%;
        }
      }
    </style>
  </body>
</html>